---
layout: home
author_profile: true
---
<head>
	<style>
		.project{
		  width: 100%;
		  margin: 50px auto;
		  overflow: hidden;
		}
		.teaser{
		  width:20%;
		  float:left;
		  padding: 0px;
		}
		.teaser img{
		  display: inline-block;
		  text-align-last: center;
/*		  width:200%;*/
		}
		.intro{
		  float: right;
		  width: 80%;
		  padding: 0px 0px 0px 20px;
		}
	</style>
</head>

<body>

<h1> Qihao Liu </h1>

<p style="text-align:justify"><small>
	Hello! I am a fourth-year Ph.D. student in Computer Science (CS) at Johns Hopkins University (JHU), advised by Prof. <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>. I hold a Master of Robotics at JHU. My research interests lie in the fields of generative models, adversarial examiners, 3D computer vision, and robustness.
</small></p>

<p><small>
	During my PhD study, I have also spent great time at ByteDance (with <a href="https://songbai.site/">Song Bai</a>, <a href="http://liangchiehchen.com/">Liang-Chieh Chen</a>), Meta GenAI (with <a href="https://ai.meta.com/people/1287460658859448/mannat-singh/">Mannat Singh</a>, <a href="https://xiyinmsu.github.io/">Xi Yin</a>, and <a href="https://www.robots.ox.ac.uk/~abrown/">Andrew Brown</a>). Before Hopkins, I received my Bachelors from Shanghai Jiao Tong University. 
</small></p>

<p><small>
	I am actively seeking full-time research opportunities in the industry, beginning in Summer or Winter 2025. If you're aware of any suitable positions, I would greatly appreciate it if you could reach out to me.
</small></p>


<h2> Research Interests </h2>
<hr />
<small><p>
Over the past few years, I have delved in several fascinating research topics, including generative models, adversarial examiners, and 3D computer vision. My journey began with robotic manipulation using <a href="https://arxiv.org/abs/1811.10264">reinforcement learning</a> and <a href="https://arxiv.org/abs/2012.00088">3D computer vision</a>. Recognizing robustness as a critical challenge for real-world applications of computer vision and other AI systems, I turned my focus to integrating <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">human knowledge</a>, <a href="https://arxiv.org/abs/2303.07337">graph-based models</a>, and <a href="https://arxiv.org/abs/2306.08103">generative models</a> to enhance both performance and robustness of vision systems. These efforts led to the development of <a href="https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2020/Lec25/AdversarialExaminerIntro.pdf">adversarial examiner</a>, which I further applied to generative models to systematically uncover and analyze their <a href="https://arxiv.org/abs/2306.00974">failure modes</a>.
</p></small>
<small><p>
Recognizing that generative and discriminative models can enhance each other—through methods including adversarial examiners—and the relatively underexplored potential of generative models, I also focused on improving generative models for image, 3D, and video generation. For example, <a href="https://arxiv.org/abs/2406.09416">DiMR</a> progressively refines details from low to high resolution, significantly reducing distortions and enhancing visual fidelity in image generation. <a href="https://arxiv.org/pdf/2412.15213">CrossFlow</a> introduces a novel approach for T2I generation by directly mapping between modalities with flow matching, bypassing the traditional reliance on Gaussian noise. <a href="https://arxiv.org/abs/2406.04322">DIRECT-3D</a> establishes a new training paradigm for 3D generative models, enabling effective learning from large-scale, noisy, and unaligned 3D data sourced from the Internet. ReVision utilizes explicit 3D knowledge to improve video generation, particularly for generating complex motions.
</p></small>
<small><p>
I have also worked on topics including segmentation, video understanding, foundational object models, and 3D datasets during my internships and collaborations.
</p></small>
<small><p>
In short, during my PhD studies, I have been the core contributor to several projects that align closely with my research interests:
</p></small>
<!-- <small><p>My research interests lie in the fields of 3D computer vision, generative models, robustness, and robotics. Specifically, my recent work focuses on:</p></small> -->
<ul>
	<li style="margin:0px"><small>Image/3D/video generation [<a href="https://arxiv.org/abs/2306.08103">ICLR24</a>, <a href="https://arxiv.org/abs/2406.04322">CVPR24</a>, <a href="https://arxiv.org/abs/2406.09416">NeurIPS24</a>, <a href="https://arxiv.org/pdf/2412.15213">Arxiv24</a>]</small></li>
	<li style="margin:0px"><small>Evaluating and improving ML system with adversarial examiner [<a href="https://arxiv.org/abs/2303.07337">CVPR23</a>, <a href="https://arxiv.org/abs/2306.00974">ICLR24</a>]</small></li>
	<li style="margin:0px"><small>Video instance segmentation and tracking [<a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19815-1_34.pdf">ECCV22</a>, <a href="https://arxiv.org/abs/2303.08132">CVPR23</a>]</small></li>
	<li style="margin:0px"><small>3D human/articulated object pose estimation [<a href="https://arxiv.org/abs/2012.00088">Arxiv20</a>, <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">ECCV22</a>]</small></li>
	<li style="margin:0px"><small>Reinforcement learning for robot manipulation [<a href="https://arxiv.org/abs/1811.10264">IROS21</a>]</small></li>
</ul>


<h2> News </h2>
<hr />
<ul style="margin:0px auto">
	<li style="margin:0px auto"><small>[2024.12] <a href="https://cross-flow.github.io/">CrossFlow</a> is released! Code and models are available <a href="https://github.com/qihao067/CrossFlow">here</a></small></small>.</li>
	<li style="margin:0px auto"><small>[2024.09] <a href="https://imagenet3d.github.io">ImageNet3D</a> is accepted at NeurIPS D&B Track 2024.</small></li>
	<li style="margin:0px auto"><small>[2024.09] <a href="https://qihao067.github.io/projects/DiMR">DiMR</a> is accepted at NeurIPS 2024. Code is available <a href="https://github.com/qihao067/DiMR">here</a></small>.</li>
	<li style="margin:0px auto"><small>[2024.02] <a href="https://direct-3d.github.io/">DIRECT-3D</a> and <a href="https://glee-vision.github.io/">GLEE</a> are accepted at CVPR 2024, with code and models available <a href="https://github.com/qihao067/direct3d">here</a> and <a href="https://github.com/FoundationVision/GLEE">here</a></small>.</li>
	<li style="margin:0px auto"><small>[2024.01] <a href="https://sage-diffusion.github.io/">SAGE</a> and <a href="https://ccvl.jhu.edu/3D-DST/">3D-DST</a> are accepted at ICLR 2024, with 3D-DST honored as a spotlight presentation.</small></li>
	<li style="margin:0px auto"><small>[2023.07] <a href="https://xujiacong.github.io/Animal3D/">Animal3D</a> is accepted at ICCV 2023.</small></li>
	<li style="margin:0px auto"><small>[2023.02] <a href="https://arxiv.org/abs/2303.07337">PoseExaminer</a> and <a href="https://arxiv.org/abs/2303.08132">InstMove</a> are accepted at CVPR 2023.</small></li>
	<li style="margin:0px auto"><small>[2022.07] <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19815-1_34.pdf">IDOL</a> and <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">HUPOR</a> are accepted at ECCV 2022, including one oral.</small></li>
	<li style="margin:0px auto"><small>[2022.03] <a href="https://qliu24.github.io/udapart/">UDA-Part</a> is accepted at CVPR 2022 for oral presentation.</small></li>
	<li style="margin:0px auto"><small>[2021.07] <a href="https://ieeexplore.ieee.org/abstract/document/9636234/">PNS</a> is accepted at IROS 2021.</small></li>
	<li style="margin:0px auto"><small>[2021.04] I will join the Computational Cognition, Vision, and Learning (CCVL) Lab as a Ph.D. student at Johns Hopkins University in August 2021.</small></li>
</ul>

<h2> Publications </h2>
<hr />
<b> Selected Papers: </b>


<!-- CrossFlow -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/crossflow.gif"/>
	</div>
	<div class="intro">
		<font size=4><b> Flowing from Words to Pixels: A Framework for Cross-Modality Evolution </b></font><br />
		<small> <b>Qihao Liu</b>, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh </small><br />
		<small> Submitted to CVPR 2025 </small><br />
		<small> [TL;DR] CrossFlow is a simple, general framework that maps between two modalities using standard flow matching without additional conditioning, achieving SOTA results across tasks like T2I, depth estimation, and image captioning—without cross-attention or task-specific designs.  </small><br />
		<small>( <a href="https://arxiv.org/pdf/2412.15213">arxiv</a> | <a href="https://cross-flow.github.io/">project page</a> | <a href="https://github.com/qihao067/CrossFlow">code</a> ) </small>
	</div>
</div>


<!-- ReVision -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/ReVision.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> ReVision: Regenerate Video with Explicit 3D Optimization on Implicit Skinned Motion for Consistent Image-to-video Generation </b></font><br />
		<small> <b>Qihao Liu</b>, Ju He, Jiawei Peng, Qihang Yu, Liang-Chieh Chen, Alan Yuille </small><br />
		<small> Submitted to CVPR 2025 </small><br />
		<small> [TL;DR] ReVision is a three-stage image-to-video framework that uses explicit 3D priors to enhance pre-trained models like Stable Video Diffusion, enabling high-quality motion generation with precise control over details like eyes and hands.  </small><br />
		<!-- <small>( <a href="https://arxiv.org/pdf/2412.15213">arxiv</a> | <a href="https://cross-flow.github.io/">project page</a> | <a href="https://github.com/qihao067/CrossFlow">code</a> ) </small> -->
		<small> paper and code comming soon </small>
	</div>
</div>


<!-- DiMR -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/DiMR.jpeg"/>
	</div>
	<div class="intro">
		<font size=4><b> Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models </b></font><br />
		<small> <b>Qihao Liu*</b>, Zhanpeng Zeng*, Ju He*, Qihang Yu, Xiaohui Shen, Liang-Chieh Chen </small><br />
		<small> Neural Information Processing Systems (NeurIPS), 2024 </small><br />
		<small> [TL;DR] DiMR is a new diffusion backbone that achieves state-of-the-art image generation. For example, on the ImageNet 256 x 256 benchmark, DiMR with only 505M parameters surpasses all existing image generation models of various sizes, without any bells and whistles.  </small><br />
		<small>( <a href="https://arxiv.org/abs/2406.09416">arxiv</a> | <a href="https://qihao067.github.io/projects/DiMR">project page</a> | <a href="https://github.com/qihao067/DiMR">code</a> ) </small>
	</div>
</div>



<!-- DIRECT-3D -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/DIRECT-3D.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data </b></font><br />
		<small> <b>Qihao Liu</b>, Yi Zhang, Song Bai, Adam Kortylewski, Alan Yuille </small><br />
		<small> Computer Vision and Pattern Recognition Conference (CVPR), 2024 </small><br />
		<small> [TL;DR] DIRECT-3D is a new text-to-3D generative model that directly generates 3D contents in a single forward pass without optimization. It also provides accurate and effective 3D geometry prior for other tasks. </small><br />
		<small>( <a href="https://arxiv.org/abs/2406.04322">arxiv</a> | <a href="https://direct-3d.github.io/">project page</a> | <a href="https://github.com/qihao067/direct3d">code</a> ) </small>
	</div>
</div>


<!-- DST -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/DST.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> Generating Images with 3D Annotations using Diffusion Models</b></font><br />
		<small> Wufei Ma*, <b>Qihao Liu*</b>, Jiahao Wang*, Xiaoding Yuan, Angtian Wang, Yi Zhang, Zihao Xiao, Guofeng Zhang, Beijia Lu, Ruxiao Duan, Yongrui Qi, Adam Kortylewski, Yaoyao Liu, Alan Yuille </small><br />
		<small> (*Equal contribution) </small><br />
		<small> International Conference on Learning Representations (ICLR), Spotlight (Top5%), 2024 </small><br />
		<small> [TL;DR] 3D-DST simplifies 3D geometry control in diffusion models, enabling structural edits and automatic 3D annotations in generated images. These images improve various 2D and 3D tasks, like classification and pose estimation, in both in-distribution and out-of-distribution settings. </small><br />
		<small>( <a href="https://arxiv.org/abs/2306.08103">arxiv</a> | <a href="https://ccvl.jhu.edu/3D-DST/">project page</a> | <a href="https://github.com/wufeim/DST3D">code</a> |  <a href="https://huggingface.co/datasets/ccvl/DST-3D">dataset</a> ) </small>
	</div>
</div>

<!-- SAGE -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/SAGE.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search</b></font><br />
		<small> <b>Qihao Liu</b>, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille </small><br />
		<small> International Conference on Learning Representations (ICLR), 2024 </small><br />
		<small> [TL;DR] SAGE is the first automated method to systematically uncover failure modes in diffusion models by exploring discrete prompt and high-dimensional latent spaces. We analyze failure modes in SOTA generative models, revealing four unexpected behaviors not previously studied systematically. </small><br />
		<small>( <a href="https://arxiv.org/abs/2306.00974">arxiv</a> | <a href="https://sage-diffusion.github.io/">project page</a> )</small>
	</div>
</div>


<!-- PoseExaminer -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/PoseExaminer.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> PoseExaminer: Automated Testing of OOD Robustness in Human Pose and Shape Estimation </b></font><br />
		<small> <b>Qihao Liu</b>, Adam Kortylewski, Alan Yuille </small><br />
		<small> Computer Vision and Pattern Recognition Conference (CVPR), 2023 </small><br />
		<small> [TL;DR] PoseExaminer is an automated tool for evaluating HPS methods, identifying failure modes to assess performance and robustness. Fine-tuning based on these failure modes significantly boosts both robustness and performance on standard benchmarks.</small><br />
		<small> ( <a href="https://arxiv.org/abs/2303.07337">arxiv</a> | <a href="https://github.com/qihao067/PoseExaminer">code</a> ) </small>
		
	</div>
</div>


<!-- InstMove -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/InstMove.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> InstMove: Instance Motion for Object-centric Video Segmentation </b></font><br />
		<small> <b>Qihao Liu*</b>, Junfeng Wu*, Yi Jiang, Xiang Bai, Alan Yuille, Song Bai</small><br />
		<small> (*Equal contribution) </small><br />
		<small> Computer Vision and Pattern Recognition Conference (CVPR), 2023 </small><br />
		<small> [TL;DR] InstMove predicts instance-level motion and deformation from previous instance masks, providing accurate and robust object-level prior that enhances video segmentation, even in cases of occlusion and rapid motion.</small><br />
		<small> ( <a href="https://arxiv.org/abs/2303.08132">arxiv</a> | <a href="https://github.com/wjf5203/VNext/tree/main/projects/InstMove">code</a> )  </small>
		
	</div>
</div>

<!-- IDOL -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/idol.gif"/>
	</div>
	<div class="intro">
		<font size=4><b> In Defense of Online Models for Video Instance Segmentation</b></font><br />
		<small> Junfeng Wu*, <b>Qihao Liu*</b>, Yi Jiang, Song Bai, Alan Yuille, Xiang Bai</small><br />
		<small> (*Equal contribution) </small><br />
		<small> European Conference on Computer Vision (ECCV), Oral, 2022 </small><br />
		<small> [TL;DR] We identify the main cause of the performance gap between online and offline VIS models and propose IDOL, a contrastive learning-based online framework that, for the first time, surpasses all offline methods across three benchmarks.</small><br />
		<small>( <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-19815-1_34.pdf">PDF</a> | <a href="https://github.com/wjf5203/VNext">code (&#9734 605)</a> | <a href="https://youtube-vos.org/assets/challenge/2022/reports/VIS_1st.pdf">1-st place solution for CVPR2022 VIS workshop</a> )</small>
	</div>
</div>

<!-- HUPOR -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/hupor.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation</b></font><br />
		<small> <b>Qihao Liu</b>, Yi Zhang, Song Bai, Alan Yuille </small><br />
		<small> European Conference on Computer Vision (ECCV), 2022 </small><br />
		<small> [TL;DR] HUPOR leverages visible cues to infer occluded joints, significantly improving bottom-up multi-person human pose estimation, even in occluded scenarios. </small><br />
		<small>( <a href="https://link.springer.com/content/pdf/10.1007/978-3-031-20065-6_29.pdf">PDF</a> | <a href="https://github.com/qihao067/HUPOR">code</a> )</small>
	</div>
</div>

<!-- GC-POSE -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/GC-Pose.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> Nothing but Geometric Constraints: A Model-free Method for Articulated Object Pose Estimation</b></font><br />
		<small> <b>Qihao Liu</b>, Weichao Qiu, Weiyao Wang, Gregory Hager, Alan Yuille </small><br />
		<small>( <a href="https://arxiv.org/abs/2012.00088">arxiv</a> )</small>
	</div>
</div>

<!-- PNS -->
<div class="project">
	<div class="teaser">
		<img src="/imgs/pns.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> PNS: Population-Guided Novelty Search for Reinforcement Learning in Hard Exploration Environments</b></font><br />
		<small> <b>Qihao Liu</b>, Yujia Wang, Xiaofeng Liu </small><br />
		<small> International Conference on Intelligent Robots and Systems (IROS) 2021 </small><br />
		<small>( <a href="https://ieeexplore.ieee.org/abstract/document/9636234/">PDF</a> | <a href="https://arxiv.org/pdf/1811.10264.pdf">arxiv</a> )</small>
	</div>
</div>


<!-- <b> Other Papers: </b>
<div class="project">
	<div class="teaser">
		<img src="/imgs/UDA.jpg"/>
	</div>
	<div class="intro">
		<font size=4><b> Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles</b></font><br />
		<small> Qing Liu, Adam Kortylewski, Zhishuai Zhang, Zizhang Li, Mengqi Guo, <b>Qihao Liu</b>, Xiaoding Yuan, Jiteng Mu, Weichao Qiu, Alan Yuille </small><br />
		<small> Computer Vision and Pattern Recognition Conference (CVPR), Oral, 2022 </small><br />
		<small>( <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Part_Segmentation_Through_Unsupervised_Domain_Adaptation_From_Synthetic_Vehicles_CVPR_2022_paper.pdf">PDF</a> | <a href="https://qliu24.github.io/udapart/">dataset</a> )</small>
	</div>
</div> -->


<!-- <zero-md src="2009-05-15-edge-case-nested-and-mixed-lists.md"></zero-md> -->


<!-- <ul>
	{% for post in site.posts %}
		<li>
			<a href="{{ post.url 2009-05-15-edge-case-nested-and-mixed-lists}}">{{ Edge Case: Nested and Mixed Lists }}</a>
		</li>
	{% endfor %}
</ul> -->

</body>
